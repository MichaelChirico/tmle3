---
title: "test"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown


```{r}
passes <- c()
passes1 <- c()
passes2 <- c()

for(i in 1:1){
  print(i)

n <- 500
W <- runif(n, -1, 1)
A <- rbinom(n, size = 1, prob = plogis(W))
Y <-  rbinom(n, size = 1, prob = plogis(W)) #rnorm(n, mean =  A*W + A+W, sd = 0.3)
data <- data.table(W,A,Y)
lrnr_Y0W <- Lrnr_gam$new()
lrnr_A <- Lrnr_gam$new()
 
node_list <- list (W = "W", A = "A", Y= "Y")
learner_list <- list(A  = lrnr_A, Y = lrnr_Y0W )
# spec_spCATE <- tmle3_Spec_spCausalGLM$new(~1, "CATE")
# out <- tmle3(spec_spCATE, data, node_list, learner_list = learner_list) 
spec_spCATE <- tmle3_Spec_npCausalGLM$new(~1 + W, "CATE")
suppressWarnings(out <- tmle3(spec_spCATE, data, node_list, learner_list = learner_list) )
 out <- out$summary
passes <- cbind(passes , out$lower <= 1 & out$upper >= 1)


spec_spCATE <- tmle3_Spec_npCausalGLM$new(~1 + W, "CATT")
suppressWarnings(out <- tmle3(spec_spCATE, data, node_list, learner_list = learner_list) )
 out <- out$summary
passes1 <- cbind(passes1 , out$lower <= 1 & out$upper >= 1)


spec_spCATE <- tmle3_Spec_spCausalGLM$new(~1 + W, "CATE")
suppressWarnings(out <- tmle3(spec_spCATE, data, node_list, learner_list = learner_list) )
 out <- out$summary
passes2 <- cbind(passes2 , out$lower <= 1 & out$upper >= 1)

print(rowMeans(passes))
print(rowMeans(passes1))
print(rowMeans(passes2))
}
```

 
 

```{r, include = T}

passes <- c()
passes1 <- c()
for(i in 1:1){
  print(i)
library(sl3)
n <- 500
W <- runif(n, -1, 1)
A <- rbinom(n, size = 1, prob = plogis(0))
Y <-  rbinom(n, size = 1, prob = plogis(A + W + A*W))
 quantile(plogis(1 + W) * (1-plogis(1 + W)) / ( plogis( W) * (1-plogis( W))))
data <- data.table(W,A,Y)
lrnr_Y0W <- Lrnr_glm$new()
lrnr_A <- Lrnr_glm$new()
node_list <- list (W = "W", A = "A", Y= "Y")
learner_list <- list(A  = lrnr_A, Y = lrnr_Y0W)
spec_spCATE <- tmle3_Spec_spCausalGLM$new(~1 + W, "OR")
 suppressWarnings(out <- tmle3(spec_spCATE, data, node_list, learner_list = learner_list))
 out <- out$summary
passes <- cbind(passes , out$lower <= 1 & out$upper >= 1)
print(out)

spec_spCATE <- tmle3_Spec_npCausalGLM$new(~1 + W, "OR")
suppressWarnings(out <- tmle3(spec_spCATE, data, node_list, learner_list = learner_list) )
 out <- out$summary
passes1 <- cbind(passes1 , out$lower <= 1 & out$upper >= 1)
print(out)
print(rowMeans(passes))
print(rowMeans(passes1))
}
```

```{r}
#' library(glmnet)
#' n <- 200
#' W <- runif(n, -1, 1)
#' A <- rbinom(n, 1, plogis(W))
#' Y_continuous <- rnorm(n, mean =  A+W, sd = 0.3)
#' Y_binary <- rbinom(n, 1, plogis(A + W))
#' Y_count <- rpois(n, exp(A+W))
#' data <- data.table(W,A,Y_continuous, Y_binary, Y_count)

#' # Make tasks
#' task_continuous <- sl3_Task$new(data, covariates = c("A", "W"), outcome = "Y_continuous")
#' task_binary <- sl3_Task$new(data, covariates = c("A", "W"), outcome = "Y_binary")
#' task_count <- sl3_Task$new(data, covariates = c("A", "W"), outcome = "Y_count", outcome_type = "continuous")
#' 
#' formula_sp <- ~ 1 + W
#' 
# fit partially-linear least-squares regression with `append_interaction_matrix = TRUE`
#' set.seed(100)
#' lrnr_baseline <- Lrnr_glmnet$new()
#' family <- gaussian()
#' lrnr_glm_sp_gaussian <- Lrnr_glm_semiparametric$new(formula_sp = formula_sp, family = family, lrnr_baseline = lrnr_baseline, interaction_variable = "A", append_interaction_matrix = TRUE)
#' lrnr_glm_sp_gaussian <- lrnr_glm_sp_gaussian$train(task_continuous)
#' preds <- lrnr_glm_sp_gaussian$predict(task_continuous)
#' beta <- lrnr_glm_sp_gaussian$fit_object$coefficients
## In this case, since `append_interaction_matrix = TRUE`, it is equivalent to:
#' V <- model.matrix(formula_sp, task_continuous$data)
#' X <- cbind(task_continuous$data[["W"]], task_continuous$data[["A"]]*V)
#' X0 <- cbind(task_continuous$data[["W"]], 0*V)
#' colnames(X) <- c("W", "A", "A*W")
#' Y <- task_continuous$Y
#' set.seed(100)
#' beta_equiv <- coef(cv.glmnet(X, Y, family = "gaussian"), s = "lambda.min")[c(3,4)] 
#' print(beta - beta_equiv) ## Actually, the glmnet fit is projected onto the semiparametric model using 
#' glm.fit. This has no effect on glmnet since it is linear. For nonlinear learners like Lrnr_gam or Lrnr_xgboost this projection has an effect.
#' 
#' # fit partially-linear least-squares regression with `append_interaction_matrix = FALSE`
#' set.seed(100)
#' lrnr_baseline <- Lrnr_glm$new(family = gaussian())
#' family <- gaussian()
#' lrnr_glm_sp_gaussian <- Lrnr_glm_semiparametric$new(formula_sp = formula_sp, family = family, lrnr_baseline = lrnr_baseline, interaction_variable = "A", append_interaction_matrix = FALSE)
#' lrnr_glm_sp_gaussian <- lrnr_glm_sp_gaussian$train(task_continuous)
#' preds <- lrnr_glm_sp_gaussian$predict(task_continuous)
#' beta <- lrnr_glm_sp_gaussian$fit_object$coefficients
#'  
## In this case, since `append_interaction_matrix = TRUE`, it is equivalent to:
#' subset_to <- task_continuous$data[["A"]]==0 # Subset to baseline treatment arm
#' V <- model.matrix(formula_sp, task_continuous$data)  
#' X <- cbind(rep(1,n), task_continuous$data[["W"]])   
#' Y <- task_continuous$Y
#' set.seed(100)
#' beta_Y0W <- lrnr_glm_sp_gaussian$fit_object$lrnr_baseline$fit_object$coefficients
#' beta_Y0W_equiv <- coef(glm.fit(X[subset_to,,drop=F],   Y[subset_to], family = gaussian()))  # Subset to baseline treatment arm
#' EY0 <- X %*% beta_Y0W
#' beta_equiv <- coef(glm.fit(A*V, Y, offset = EY0, family = gaussian())) 
#' print(beta_Y0W - beta_Y0W_equiv)
#' print(beta - beta_equiv)


 
#' # fit partially-linear logistic regression
#' lrnr_baseline <- Lrnr_glmnet$new()
#' family <- binomial()
#' lrnr_glm_sp_binomial <- Lrnr_glm_semiparametric$new(formula_sp = formula_sp, family = family, lrnr_baseline = lrnr_baseline, interaction_variable = "A", append_interaction_matrix = TRUE)
#' lrnr_glm_sp_binomial <- lrnr_glm_sp_binomial$train(task_binary)
#' preds <- lrnr_glm_sp_binomial$predict(task_binary)
#' beta <- lrnr_glm_sp_binomial$fit_object$coefficients
 

#' # fit partially-linear log-link (elative-risk) regression
#' lrnr_baseline <- Lrnr_glmnet$new(family = "poisson") # This setting requires that lrnr_baseline predicts nonnegative values. It is recommended to use poisson regression based learners. 
#' family <- poisson()
#' lrnr_glm_sp_binomial <- Lrnr_glm_semiparametric$new(formula_sp = formula_sp, family = family, lrnr_baseline = lrnr_baseline, interaction_variable = "A", append_interaction_matrix = TRUE)
#' lrnr_glm_sp_binomial <- lrnr_glm_sp_binomial$train(task_count)
#' preds <- lrnr_glm_sp_binomial$predict(task_count)
#' beta <- lrnr_glm_sp_binomial$fit_object$coefficients
 

```





```{r}
library(sl3)

passes <- c()
for(i in 1:200){
  print(i)
n <- 500
W <- runif(n, -1, 1)
A <- rbinom(n, size = 1, prob = plogis(W))
Y <-  rpois(n, exp(A  + A*W + W))
data <- data.table(W,A,Y)
data
lrnr_Y0W <- Lrnr_gam$new(family = poisson())
lrnr_A <- Lrnr_gam$new()

node_list <- list (W = "W", A = "A", Y= "Y")
learner_list <- list(A  = lrnr_A, Y = lrnr_Y0W)
spec_spCATE <- tmle3_Spec_spCausalGLM$new(~1 + W, "RR")
out <- suppressWarnings(tmle3(spec_spCATE, data, node_list, learner_list = learner_list) )
 out <- out$summary
passes <- cbind(passes , out$lower <= 1 & out$upper >= 1)
print(rowMeans(passes))

}
```
