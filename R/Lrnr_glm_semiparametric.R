#' Semiparametric Generalized Linear Models
#'
#' This learner provides fitting procedures for semiparametric generalized linear models using a user-given baseline learner and
#' \code{\link[stats]{glm.fit}}. It supports models of the form `linkfun(E[Y|A,W]) = linkfun(E[Y|A=0,W]) + A * f(W)` where `A` is a binary or continuous interaction variable,
#' and `f(W)` is a user-specified parametric function (e.g. `f(W) = model.matrix(formula_sp, W)`). The baseline function `E[Y|A=0,W]` is fit using a user-specified Learner (possibly pooled over values of `A` and then projected onto the semiparametric model).
#'
#' @docType class
#'
#' @importFrom R6 R6Class
#'
#' @export
#'
#' @keywords data
#'
#' @return A learner object inheriting from \code{\link{Lrnr_base}} with
#'  methods for training and prediction. For a full list of learner
#'  functionality, see the complete documentation of \code{\link{Lrnr_base}}.
#'
#' @format An \code{\link[R6]{R6Class}} object inheriting from
#'  \code{\link{Lrnr_base}}.
#'
#' @family Learners
#'
#' @section Parameters:
#' \describe{
#'   \item{\code{formula_sp}}{ A \code{\link{formula}} object specifying the parametric component of the semiparametric model.}
#'   \item{\code{lrnr_baseline}}{A baseline learner for estimation of the nonparametric component. This can be pooled or unpooled by specifying \code{return_matrix_predictions}}
#'   \item{\code{interaction_variable = "A"}}{A interaction variable name (that can be found in `training_task$data`) to multiply by the design matrix generated by \code{formula_sp}. If NULL then the interaction variable is treated identically `1`.
#'   In many applications, this will be the name of a binary treatment variable (e.g. `A`).}
#'   \item{\code{family = NULL}}{A family object whose link function specifies the type of semiparametric model (e.g. partially-linear least-squares (\code{\link{gaussian}), partially-linear logistic regression (\code{\link{binomial}), partially-linear log-linear regression (\code{\link{poisson}) }
#'   \item{\code{append_interaction_matrix = TRUE}}{Whether \code{lrnr_baseline} should be fit on `cbind(task$X,A*V)` where `A` is the interaction variable and `V` is the design matrix obtained from \code{formula_sp}.
#'   Note, if `append_interaction_matrix = TRUE`, the resulting estimator will be projected onto the semiparametric model using \code{glm.fit}.
#'   If this is FALSE and \code{interaction_variable} is binary then the semiparametric model is learned by stratifying on \code{interaction_variable}.
#'   Specifically, if FALSE, \code{lrnr_baseline} is used to estimate `E[Y|A=0,W]` by subsetting to only observations with `A = 0`.
#'   In the binary case, setting `append_interaction_matrix = TRUE` allows one to pool the learning across treatment arms and allows additive models to perform well.  }
#'   \item{\code{return_matrix_predictions = FALSE}}{Only used if \code{interaction_variable} is binary. Whether to return a matrix output with three columns being `E[Y|A=0,W], E[Y|A=1,W], E[Y|A,W]`.}
#'  \item{\code{...}}{Not used.}
#' }
#'
#' @examples
#' library(glmnet)
#' n <- 200
#' W <- runif(n, -1, 1)
#' A <- rbinom(n, 1, plogis(W))
#' Y_continuous <- rnorm(n, mean = A + W, sd = 0.3)
#' Y_binary <- rbinom(n, 1, plogis(A + W))
#' Y_count <- rpois(n, exp(A + W))
#' data <- data.table(W, A, Y_continuous, Y_binary, Y_count)
#'
#' # Make tasks
#' task_continuous <- sl3_Task$new(data, covariates = c("A", "W"), outcome = "Y_continuous")
#' task_binary <- sl3_Task$new(data, covariates = c("A", "W"), outcome = "Y_binary")
#' task_count <- sl3_Task$new(data, covariates = c("A", "W"), outcome = "Y_count", outcome_type = "continuous")
#'
#' formula_sp <- ~ 1 + W
#'
#' # fit partially-linear least-squares regression with `append_interaction_matrix = TRUE`
#' set.seed(100)
#' lrnr_baseline <- Lrnr_glmnet$new()
#' family <- gaussian()
#' lrnr_glm_sp_gaussian <- Lrnr_glm_semiparametric$new(formula_sp = formula_sp, family = family, lrnr_baseline = lrnr_baseline, interaction_variable = "A", append_interaction_matrix = TRUE)
#' lrnr_glm_sp_gaussian <- lrnr_glm_sp_gaussian$train(task_continuous)
#' preds <- lrnr_glm_sp_gaussian$predict(task_continuous)
#' beta <- lrnr_glm_sp_gaussian$fit_object$coefficients
#' ## In this case, since `append_interaction_matrix = TRUE`, it is equivalent to:
#' V <- model.matrix(formula_sp, task_continuous$data)
#' X <- cbind(task_continuous$data[["W"]], task_continuous$data[["A"]] * V)
#' X0 <- cbind(task_continuous$data[["W"]], 0 * V)
#' colnames(X) <- c("W", "A", "A*W")
#' Y <- task_continuous$Y
#' set.seed(100)
#' beta_equiv <- coef(cv.glmnet(X, Y, family = "gaussian"), s = "lambda.min")[c(3, 4)]
#' ## Actually, the glmnet fit is projected onto the semiparametric model with glm.fit (no effect in this case)
#' print(beta - beta_equiv)
#'
#' # fit partially-linear least-squares regression with `append_interaction_matrix = FALSE`
#' set.seed(100)
#' lrnr_baseline <- Lrnr_glm$new(family = gaussian())
#' family <- gaussian()
#' lrnr_glm_sp_gaussian <- Lrnr_glm_semiparametric$new(formula_sp = formula_sp, family = family, lrnr_baseline = lrnr_baseline, interaction_variable = "A", append_interaction_matrix = FALSE)
#' lrnr_glm_sp_gaussian <- lrnr_glm_sp_gaussian$train(task_continuous)
#' preds <- lrnr_glm_sp_gaussian$predict(task_continuous)
#' beta <- lrnr_glm_sp_gaussian$fit_object$coefficients
#' ## In this case, since `append_interaction_matrix = TRUE`, it is equivalent to:
#' ## Subset to baseline treatment arm
#' subset_to <- task_continuous$data[["A"]] == 0
#'
#' V <- model.matrix(formula_sp, task_continuous$data)
#' X <- cbind(rep(1, n), task_continuous$data[["W"]])
#' Y <- task_continuous$Y
#' set.seed(100)
#' beta_Y0W <- lrnr_glm_sp_gaussian$fit_object$lrnr_baseline$fit_object$coefficients
#' beta_Y0W_equiv <- coef(glm.fit(X[subset_to, , drop = F], Y[subset_to], family = gaussian())) # Subset to baseline treatment arm
#' EY0 <- X %*% beta_Y0W
#' beta_equiv <- coef(glm.fit(A * V, Y, offset = EY0, family = gaussian()))
#' print(beta_Y0W - beta_Y0W_equiv)
#' print(beta - beta_equiv)
#'
#' # fit partially-linear logistic regression
#' lrnr_baseline <- Lrnr_glmnet$new()
#' family <- binomial()
#' lrnr_glm_sp_binomial <- Lrnr_glm_semiparametric$new(formula_sp = formula_sp, family = family, lrnr_baseline = lrnr_baseline, interaction_variable = "A", append_interaction_matrix = TRUE)
#' lrnr_glm_sp_binomial <- lrnr_glm_sp_binomial$train(task_binary)
#' preds <- lrnr_glm_sp_binomial$predict(task_binary)
#' beta <- lrnr_glm_sp_binomial$fit_object$coefficients
#'
#' # fit partially-linear log-link (relative-risk) regression
#' lrnr_baseline <- Lrnr_glmnet$new(family = "poisson") # This setting requires that lrnr_baseline predicts nonnegative values. It is recommended to use poisson regression based learners.
#' family <- poisson()
#' lrnr_glm_sp_binomial <- Lrnr_glm_semiparametric$new(formula_sp = formula_sp, family = family, lrnr_baseline = lrnr_baseline, interaction_variable = "A", append_interaction_matrix = TRUE)
#' lrnr_glm_sp_binomial <- lrnr_glm_sp_binomial$train(task_count)
#' preds <- lrnr_glm_sp_binomial$predict(task_count)
#' beta <- lrnr_glm_sp_binomial$fit_object$coefficients
#'
#' #
Lrnr_glm_semiparametric <- R6Class(
  classname = "Lrnr_glm_semiparametric", inherit = Lrnr_base,
  portable = TRUE, class = TRUE,
  public = list(
    initialize = function(formula_sp, lrnr_baseline, interaction_variable = "A", family = NULL, append_interaction_matrix = TRUE, return_matrix_predictions = FALSE, ...) {
      params <- args_to_list()
      super$initialize(params = params, ...)
    }
  ),

  private = list(
    .properties = c("continuous", "binomial", "semiparametric", "weights"),

    .train = function(task) {
      args <- self$params
      append_interaction_matrix <- args$append_interaction_matrix
      outcome_type <- self$get_outcome_type(task)
      trt <- args$interaction_variable
      if (is.null(trt)) {
        A <- rep(1, task$nrow)
      } else {
        A <- unlist(task$get_data(, trt))
      }
      if (!all(A %in% c(0, 1)) && !is.null(trt)) {
        binary <- FALSE
      } else {
        binary <- TRUE
      }
      family <- args$family
      lrnr_baseline <- args$lrnr_baseline
      formula <- args$formula_sp
      if (is.null(family)) {
        family <- outcome_type$glm_family(return_object = TRUE)
      }
      # Interaction design matrix
      Y <- task$Y
      V <- model.matrix(formula, task$data)
      colnames(V) <- paste0("V", 1:ncol(V))

      covariates <- setdiff(task$nodes$covariates, trt)

      if (!append_interaction_matrix && binary) {
        task_baseline <- task$next_in_chain(covariates = covariates)
        lrnr_baseline <- lrnr_baseline$train(task_baseline[A == 0])
        Q0 <- lrnr_baseline$predict(task_baseline)
        beta <- suppressWarnings(coef(glm.fit(A * V, Y, offset = family$linkfun(Q0), intercept = F, weights = task$weights, family = family)))
        Q1 <- family$linkinv(family$linkfun(Q0) + V %*% beta)
        Q <- ifelse(A == 1, Q1, Q0)
      } else {
        covariates <- setdiff(task$nodes$covariates, trt)

        if (append_interaction_matrix) {
          AV <- as.data.table(A * V)
          X <- cbind(task$X[, covariates, with = F], AV)
          X0 <- cbind(task$X[, covariates, with = F], 0 * V)
        } else {
          X <- cbind(task$X[, covariates, with = F], A)
          X0 <- cbind(task$X[, covariates, with = F], A * 0)
        }


        column_names <- task$add_columns(X)
        task_baseline <- task$next_in_chain(covariates = colnames(X), column_names = column_names)

        column_names <- task$add_columns(X0)
        task_baseline0 <- task$next_in_chain(covariates = colnames(X0), column_names = column_names)

        lrnr_baseline <- lrnr_baseline$train(task_baseline)
        Q <- lrnr_baseline$predict(task_baseline)
        Q0 <- lrnr_baseline$predict(task_baseline0)
        # Project onto model

        beta <- suppressWarnings(coef(glm.fit(A * V, Q, offset = family$linkfun(Q0), intercept = F, weights = task$weights, family = family)))
      }

      fit_object <- list(
        coefficients = beta, lrnr_baseline = lrnr_baseline, covariates = covariates, family = family, formula = formula,
        append_interaction_matrix = append_interaction_matrix, binary = binary, task_baseline = task_baseline
      )
      return(fit_object)
    },
    .predict = function(task) {
      fit_object <- self$fit_object
      append_interaction_matrix <- fit_object$append_interaction_matrix
      binary <- fit_object$binary
      beta <- fit_object$coefficients
      lrnr_baseline <- fit_object$lrnr_baseline
      covariates <- fit_object$covariates
      family <- fit_object$family
      formula <- fit_object$formula

      trt <- self$params$interaction_variable
      if (is.null(trt)) {
        A <- rep(1, task$nrow)
      } else {
        A <- unlist(task$get_data(, trt))
      }
      V <- model.matrix(formula, task$data)
      colnames(V) <- paste0("V", 1:ncol(V))


      if (!append_interaction_matrix && binary) {
        task_baseline <- task$next_in_chain(covariates = covariates)
        Q0 <- lrnr_baseline$predict(task_baseline)
      } else {
        if (append_interaction_matrix) {
          X0 <- cbind(task$X[, covariates, with = F], 0 * V)
        } else {
          X0 <- cbind(task$X[, covariates, with = F], 0)
        }
        column_names <- task$add_columns(X0)
        task_baseline0 <- task$next_in_chain(covariates = colnames(X0), column_names = column_names)
        Q0 <- lrnr_baseline$predict(task_baseline0)
      }
      Q0 <- as.vector(Q0)
      Q1 <- as.vector(family$linkinv(family$linkfun(Q0) + V %*% beta))
      Q <- as.vector(family$linkinv(family$linkfun(Q0) + A * V %*% beta))
      if (self$params$return_matrix_predictions && binary) {
        predictions <- cbind(Q0, Q1, Q)
        colnames(predictions) <- c("A=0", "A=1", "A")
        predictions <- sl3::pack_predictions(predictions)
      } else {
        predictions <- Q
      }


      return(predictions)
    }
  )
)
